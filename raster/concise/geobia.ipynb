{"cells":[{"cell_type":"markdown","metadata":{"id":"biA9PeJsOilt"},"source":["<img src='https://github.com/chikaj/wekeo_competition_2022/blob/main/img/banner2.png?raw=1' align='left' width='75%'/>"]},{"cell_type":"markdown","metadata":{"id":"XylD5xfjOilu"},"source":["**Authors:** Nate Currit <br>\n","**Copyright:** 2022 Nate Currit <br>\n","**License:** MIT"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"hi7rk4G8Oilv"},"source":["<div class=\"alert alert-block alert-success\">\n","<h3>Geographic Object-Based Image Analysis (GEOBIA)</h3></div>"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"nTLx7i3KOilv"},"source":["# INTRODUCTION\n","\n","## Learning outcomes\n","\n","At the end of this notebook you will know:\n","* What GEOBIA is and what its advantages are over other image classification methods\n","* The steps of GEOBIA\n","* How to perform GEOBIA using open source tools\n","\n","\n","## Outline\n","\n","Geographic object-based image analysis is a response to pixel-based image analysis. The literature on GEOBIA is extensive, but the following peer-reviewed articles will get you started.The first two articles are conceptual and cover periods when GEOBIA was gaining broad acceptance and as it was maturing. The third article is an application that directly compares GEOBIA results to pixel-based results.\n","\n","* Blaschke, T., Hay, G.J., Kelly, M., Land, S., Hofmann, P., Addink, E., Queiroz Feitosa, R., van der Meer, F., van der Werff, H., van Coille and F, Tiede, D. (2014) <a href=\"https://www.sciencedirect.com/science/article/pii/S0924271613002220\" target=\"_blank\">Geographic Object-Based Image Analysis â€“ Towards a new paradigm</a>. ISPRS Journal of Photogrammetry and Remote Sensing. Vol. 87: 180-191.\n","* Chen, G., Weng, Q., Hay, G.J. and He, Y. (2018) <a href=\"https://www.tandfonline.com/doi/pdf/10.1080/15481603.2018.1426092\" target=\"_blank\">Geographic object-based image analysis (GEOBIA): emerging trends and future opportunities</a>. GIScience and Remote Sensing. Vol, 55, No. 2: 159-182.\n","* Facco, D.S., Guasselli, L.A., Chimelo Ruiz, L.F., Delapasse Simioni, J.P. and Dick, D.G. (2021) <a href=\"https://www.tandfonline.com/doi/abs/10.1080/10106049.2021.1899302?journalCode=tgei20\" target=\"_blank\">Comparison of PBIA and GEOBIA classification methods in classifying turbidity in reservoir</a>. GeoCarto International. Vol. 37, Iss. 16: 4762-4783.\n","\n","The rest of this notebook will be divided into the following content parts.\n","\n","<div class=\"alert alert-info\" role=\"alert\">\n","\n","### <a id='TOC_TOP'></a>Contents\n","\n","</div>\n","    \n"," 1. [Background](#background)\n"," 2. [Segmentation](#segmentation)\n"," 3. [Attribution](#attribution)\n"," 4. [Classification](#classification)\n"," 5. [Challenge](#challenge)\n","\n","\n","<hr>"]},{"cell_type":"markdown","metadata":{"id":"dVVD36nHOilw"},"source":["<div class=\"alert alert-info\" role=\"alert\">\n","\n","### <a id='background'></a>1. Background\n","A brief introduction to GEOBIA.\n","    \n","[Back to top](#TOC_TOP)\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"8M4FgxTgOilw"},"source":["### High resolution vs Low resolution imagery\n","Orbital and sub-orbital imagery has 4 unique kinds of resolution: spatial, spectral, radiometric and temporal. Spatial resolution is the one-sided distance of a ground resolution cell, or pixel, measured using a linear unit (i.e., meters). High spatial resolution imagery has smaller pixels than coarse spatial resolution imagery. This section presents new ways to consider spatial resolution in the context of GEOBIA.\n","\n","The previous definitions of spatial resolution focus on inherent characteristics of the imagery itself, but pixel size also has an important relationship with the objects of interest on the ground. **H-resolution** and **L-resolution** refer to situations when image pixels are smaller or larger, respectively, than objects of interest on the ground. That is, an H-resolution image will have objects composed of multiple image pixels. An L-resolution image will have multiple objects per pixel. Most images will contain large and small objects so this conceptualization of spatial resolution varies across an image and between different types of images.\n","\n","### Pixel-based vs Object-based classifications\n","Two broad types of image classification algorithms exist: pixel-based and object-based. In **pixel-based** classifications, individual pixels are classified indepent of neighboring pixels--there is no concept of a neighborhood of pixels that comprise a single object. These algorithms often produce 'noisy' classifications where individual pixels may be classified differently than their surrounding neighbors because of within class reflectance variations. Moreover, these algorithms may be computational inefficient because they classify multiple neighboring pixels individually even though they are all part of a single object.\n","\n","**Object-based** classification algorithms are a response to the shortcomings of pixel-based classification algorithms that recognize spatial relationships between neighboring pixels. GEOBIA is the term used for object-based classification with Earth Observation imagery. GEOBIA is used to classify H-resolution imagery, but does not require high spatial resolution imagery. GEOBIA consists of the following three steps that are further explained and demonstrated in the next sections of the Jupyter notebook.\n","1. Segmentation\n","2. Attribution (or feature extraction)\n","3. Classification\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yZlu2az7Oilx"},"source":["### Notebook preparation\n","\n","In an effort to keep this notebook free of clutter, all import statements and most of the functions are contained in a separate notebook. Executing the cell below will run those functions as if they were included in this notebook.\n","\n","*The first line of the functions notebook installs some packages not in the current WEkEO base conda environment. If it fails to install when run from this notebook, open the other notebook and run its first line. Then restart the kernel on this notebook before proceeding. If you are NOT running this on the WEkEO platform or using a package manager besides conda, please comment out the first line of the functions notebook and instead use requirements.txt to install required packages.*"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"eTef5lO6WyB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd drive/MyDrive/concise"],"metadata":{"id":"ibnNnCZxYLXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from google.colab import files\n","# uploaded = files.upload()"],"metadata":{"id":"LFs0CGMdVPD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%run ./functions.ipynb"],"metadata":{"id":"6ND9ip8BXXon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"pbGhRH4sOily"},"source":["### Data for this tutorial\n","\n","The image used for this notebook is a Sentinel 2, level 2A multispectral image. The spatial resolution is 10 meters and the spectral resolution is 4 bands: blue, green, red, and near-infrared. The data and processessing steps are fully explained in the links below. For this notebook, the subset image is located in the data folder.\n","\n","| Product Description | Data Store collection ID| Product User Guide | WEkEO HDA ID | WEkEO metadata |\n","|:--------------------:|:-----------------------:|:-------------:|:-----------------:|:-----------------:|\n","| Sentinel-2 MSI level-2A | EO:ESA:DAT:Sentinel-2:MSI | <a href=\"https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi\" target=\"_blank\">link</a> | EO:ESA:DAT:SENTINEL-2:MSI | <a href=\"https://www.wekeo.eu/data?view=viewer&t=1662249600000&z=0&center=1.29067%2C25.36539&zoom=10.57&layers=W3siaWQiOiJjNCIsImxheWVySWQiOiJFTzpFU0E6REFUOlNFTlRJTkVMLTI6TVNJL19fREVGQVVMVF9fLzFfVFJVRV9DT0xPUiIsInpJbmRleCI6MTB9XQ%3D%3D\" target=\"_blank\">link</a> |"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"KU5TRTaDOily"},"source":["#### Image access and exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-WB9N1KOily"},"outputs":[],"source":["# Open image for visualization and analysis as a datasetreader object using rasterio\n","src = rasterio.open(\"data/concise.tif\")\n","# Read the raw data as a numpy array\n","image = src.read()"]},{"cell_type":"markdown","metadata":{"id":"8SicvHtPOily"},"source":["#### Let's learn a little bit about this image by exploring some of its metadata."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVLhtwiIOilz"},"outputs":[],"source":["print(f\"Number of bands: {src.count}\")\n","print(f\"Number of rows/columns: {src.height} / {src.width}\")\n","print(f\"Pixel data types per band: {src.dtypes}\")\n","print(f\"Band 1 (blue) min/max: {image[0].min()} / {image[0].max()}\")\n","print(f\"Band 2 (green) min/max: {image[1].min()} / {image[1].max()}\")\n","print(f\"Band 3 (red) min/max: {image[2].min()} / {image[2].max()}\")\n","print(f\"Band 4 (nir) min/max: {image[3].min()} / {image[3].max()}\")"]},{"cell_type":"markdown","metadata":{"id":"CmFuSv9lOilz"},"source":["#### The image file pixel values shown above are scaled values. Dividing them by 100 produces values representative of the percent reflectance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rc9_gcerOilz"},"outputs":[],"source":["print(f\"Band 1 (blue) min/max: {image[0].min()/100}% / {image[0].max()/100}%\")\n","print(f\"Band 2 (green) min/max: {image[1].min()/100}% / {image[1].max()/100}%\")\n","print(f\"Band 3 (red) min/max: {image[2].min()/100}% / {image[2].max()/100}%\")\n","print(f\"Band 4 (nir) min/max: {image[3].min()/100}% / {image[3].max()/100}%\")"]},{"cell_type":"markdown","metadata":{"id":"8wWA9l78Oil0"},"source":["#### For a few reasons, including for visualization using Python packages, it is best to have pixel values as ratios scaled from 0 to 1, so we divide the original values by 10,000."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZNj99IWOil0"},"outputs":[],"source":["image = image / 10000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3ncW475Oil0"},"outputs":[],"source":["print(f\"Band 1 (blue) min/max: {image[0].min()} / {image[0].max()}\")\n","print(f\"Band 2 (green) min/max: {image[1].min()} / {image[1].max()}\")\n","print(f\"Band 3 (red) min/max: {image[2].min()} / {image[2].max()}\")\n","print(f\"Band 4 (nir) min/max: {image[3].min()} / {image[3].max()}\")"]},{"cell_type":"markdown","metadata":{"id":"fv_9-KctOil0"},"source":["#### Now, let's view and interpret the image!\n","\n","This is an image of a western shore of Lake Neuchatel in Switzerland. The community to the south is Concise and the community to the north is Vaumarcus. The image is a true-color composite that has been linearly stretched for better visualization. The dark green patches that run east-west through the middle of the image are forest. Agricultural fields at various stages of growth are found in unforested areas. Some variations in color are seen in the lake, depending on depth and sediment load. The urban areas are small and consist of features that are smaller than a 10 meter Sentinel pixel. Urban areas are surrounded by agricultural lands."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeNSNUwjOil1"},"outputs":[],"source":["# The following 2 lines \"stretch\" pixels between the middle 95% of pixel values\n","p25, p975 = np.percentile(image, (2.5, 97.5))\n","rgb = exposure.rescale_intensity(image, in_range=(p25, p975))\n","\n","fig, ax = plt.subplots(figsize=(14,10))\n","img_extent=[src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3]]\n","ax.use_sticky_edges = True\n","ax.imshow(bsq_to_bip(rgb_composite(rgb, 2, 1, 0)), extent=img_extent)"]},{"cell_type":"markdown","metadata":{"id":"u-1hqhcnOil1"},"source":["#### Interpreting the histogram\n","\n","The red, green, and blue histograms correspond to the red, green and blue wavelength bands of the image. The solid black line in the histogram for the near-infrared wavelength band. Each band has a peaked mode in the histogram to the left of the figure, representing low reflectance pixels corresponding to the lake. The higher reflectance pixels to the right of the histogram are more dispered (more or less depending on the band) and correspond to the different land cover types. The near-infrared band has the greatest land-cover dispersion of pixel values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvlsgF6vOil1"},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize=(10,5))\n","ax.hist(image[2].flatten(), 50, density=True, histtype='stepfilled', facecolor='r', ec='r', alpha=0.75)\n","ax.hist(image[0].flatten(), 50, density=True, histtype='stepfilled', facecolor='b', ec='b', alpha=0.75)\n","ax.hist(image[1].flatten(), 50, density=True, histtype='stepfilled', facecolor='g', ec='g', alpha=0.75)\n","ax.hist(image[3].flatten(), 50, density=True, histtype='step', ec='k', alpha=0.75)\n","ax.set_title(\"Pixel distributions per color band\")"]},{"cell_type":"markdown","metadata":{"id":"N24LugUiOil1"},"source":["<div class=\"alert alert-info\" role=\"alert\">\n","\n","### <a id='segmentation'></a>2. Segmentation\n","[Back to top](#TOC_TOP)\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"iwtxUVyuOil2"},"source":["Segmentation is the first step that distinguishes GEOBIA from pixel-based classification algorithms. Segmentation is the process of grouping pixels that comprise individual image objects and is the most critical part of the GEOBIA process. Segmentation groups similar pixels into clusters of pixels that are identified as single entities."]},{"cell_type":"markdown","metadata":{"id":"725kHEvWOil2"},"source":["#### Select algorithm and set parameters"]},{"cell_type":"markdown","metadata":{"id":"ksrJmyvxOil2"},"source":["There are many segementation algorithms that can be selected using the [scikit-image](https://scikit-image.org) package, including the Simple Linear Iterative Clustering (SLIC), Felzenszwalb, Watershed and Quickshift algorithms. It the tutorial materials below, we will use the Quickshift algorithm. In the challenge section at the of the tutorial you will have the opportunity to try other algorithms.\n","\n","Each algorithm has unique parameters that you can alter. Indeed, an analyst's task is to select parameters that partition the image into meaningful segments that can later be classified. Determining optimum parameters is usually done by trial and error and varies from image to image depending on location, illumination, image contrast, etc. The following Quickshift parameters are the ones used for this notebook. (Don't alter these now, you will be able to later.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAD4qT2IOil2"},"outputs":[],"source":["quickshift_params = {\n","    'ratio': 0.5,           # default = 1\n","    'kernel_size': 2,       # default = 5\n","    'max_dist': 5,          # default = 10\n","    'sigma': 0.2,           # default = 0\n","    'convert2lab': True,    # default = True\n","    'channel_axis': 2       # for our RGB image this should always be 2\n","}"]},{"cell_type":"markdown","metadata":{"id":"S9RpO_hYOil2"},"source":["#### Segment image\n","\n","The Quickshift parameters above are passed to the segmentation function below. The function accepts a quickshift model and an RGB color composite of our image. The output (rout) is a Numpy array (i.e., a raster)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqRE2jizOil3"},"outputs":[],"source":["# Quickshift\n","print(f\"Starting segmentation.\", end=\"...\")\n","rout = segmentation(model=quickshift, params=quickshift_params, image=image[:3])\n","print(f\"done.\")"]},{"cell_type":"markdown","metadata":{"id":"ZwonmXI4Oil3"},"source":["#### Vectorize raster segments\n","\n","The raster output from above is passed to the vectorization function in this step. The output of this function (vout) is a GeoPandas dataframe (i.e., a polygon vector) where each polygon represents a cluster of pixels representative of image objects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRLT8FylOil3"},"outputs":[],"source":["print(f\"Starting vectorization.\", end=\"...\")\n","vout = vectorize(image=rout, transform=src.transform, crs=src.crs.to_epsg())\n","print(f\"done.\")"]},{"cell_type":"markdown","metadata":{"id":"aAVgbTsfOil3"},"source":["#### Visualize segmentation\n","\n","Now we can visualize the segments overlaid on the image. Over the land you are able to see many segments that correspond to field or forest boundaries. Over the lake, you see segments that correspond to different colors of water attributable to different sediment loads. At this point, the segments are only geometries, they do not have any attributes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I53a3O6IOil3"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(14,10))\n","img_extent=[src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3]]\n","ax.use_sticky_edges = True\n","ax.imshow(bsq_to_bip(rgb_composite(rgb, 2, 1, 0)), extent=img_extent)\n","vout.boundary.plot(ax=ax)"]},{"cell_type":"markdown","metadata":{"id":"y5z8teuOOil4"},"source":["<div class=\"alert alert-info\" role=\"alert\">\n","\n","## <a id='attribution'></a>3. Attribution or Feature Extraction\n","[Back to top](#TOC_TOP)\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"Xx4dcexfOil4"},"source":["Attribution or feature extraction is another step that distinguishes GEOBIA from pixel-based classification algorithms. Each of the image segment geometries needs attributes that can later be classified. The attributes can be anything found at the location of the object geometry, but they generally fall into 3 categories: spectral, shape and texture.\n","\n","#### Spectral\n","Spectral attributes are zonal statistics of the original spectral image bands based on the vector segments derived above. For example, the average blue, green, red and near infrared pixels values of each segment are recorded as unique fields in the vector segments file. Any zonal statistic could be used: mean, minimum, maximum, standard deviation, etc.\n","\n","#### Shape\n","Geographic objects in a region may have unique shapes, depending on the resolution characteristics of the imagery. For example, tree crowns may be round, homes may be square, lakes may be elongated, etc. Depending on the objects visible in the imagery and the shape of the segments derived from the previous steps, an analyst can choose from a variety of shape parameters. Some of the shape features we extract from the geometries are area, perimeter, major axis length and orientation. Many more are part of the [scikit-image](https://scikit-image.org) package.\n","\n","#### Texture\n","Texture describes the pixel-to-pixel variation within an object. Texture is a number calculated at the image object level that indicates whether the object is 'smooth' or 'rough'. One of the most well-known texture measures is the Gray Level Cooccurance Matrix (GLCM), but it is computationally intensive to calculate it.  A simple (and important for this case--fast) method to calculate texture is to apply an edge detection algorithm to the image and then to calculate the mean and standard deviation per segment."]},{"cell_type":"markdown","metadata":{"id":"1oDSXx4mOil4"},"source":["### Spectral attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qkY1HSyOil4"},"outputs":[],"source":["print(f\"Starting add zonal spectral properties.\", end=\"...\")\n","vout = add_zonal_properties(image=image, transform=src.transform,\n","                            band_names=['blue', 'green', 'red', 'nir'],\n","                            gdf=vout)\n","print(f\"done.\")"]},{"cell_type":"markdown","metadata":{"id":"0zBX-VSLOil4"},"source":["### Shape attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6dia57rOil4"},"outputs":[],"source":["print(f\"Starting add shape properties.\", end=\"...\")\n","vout = add_shape_properties(rout, vout, ['area', 'perimeter',\n","                                        'eccentricity',\n","                                        'equivalent_diameter',\n","                                        'major_axis_length',\n","                                        'minor_axis_length',\n","                                        'orientation'])\n","print(f\"done.\")"]},{"cell_type":"markdown","metadata":{"id":"bCgF8jZtOimC"},"source":["### Texture attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wv1-028kOimC"},"outputs":[],"source":["print(f\"Starting edge detection.\", end=\"...\")\n","edges = sobel_edge_detect(image[3])\n","print(f\"done.\")\n","\n","print(f\"Starting add zonal texture properties.\", end=\"...\")\n","vout = add_zonal_properties(image=edges, band_names=['edges'],\n","                            stats=['mean', 'std'],\n","                            transform=src.transform, gdf=vout)\n","print(f\"done.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBz3I-HyOimC"},"outputs":[],"source":["vout.head()"]},{"cell_type":"markdown","metadata":{"id":"udgHu3QfOimC"},"source":["#### Review of attributes\n","\n","What we see above is the first 5 segments with their associated attributes. For each segment there are values representative of the mean blue, green, red and near-infrared pixels within the segment. There are 7 shape parameters for each segment, including area, perimeter, eccentricity, equivalent_diameter, major_axis_length, minor_axis_length, and orientation. The two texture measures are the mean and standard deviation of the pixels from a Sobel edge detection filter. These are the attributes that can be used to classify each segment as described in the next section."]},{"cell_type":"markdown","metadata":{"id":"a5ge6BSgOimC"},"source":["<div class=\"alert alert-info\" role=\"alert\">\n","\n","## <a id='classification'></a>4. Classification\n","[Back to top](#TOC_TOP)\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"n2JUZGcnOimC"},"source":["It would take many Jupyter notebooks to describe the breadth of classification algorithms available in [scikit-learn](https://scikit-learn.org/stable/index.html). We won't try to cover them all here. We will stick with one that is quite commonly used: the Random Forests algorithm. Random Forests is an ensemble of decision tree algorithms, and decision trees are non-parametric classifiers. Read all about them at [scikit-learn](https://scikit-learn.org/stable/index.html)! For our purposes now regarding GEOBIA, we will use Random Forests to classify the segments by their attributes. But first, there actually is one more attribute to join to our segments: ground reference labels."]},{"cell_type":"markdown","metadata":{"id":"glmECIMVOimD"},"source":["#### Ground reference labels\n","\n","Ground reference points are stored in a GeoPackage in the data folder. There are five classes of points (i.e., five labels) included in the file, specifically forest, grassland-green, grassland-light, lake-deep and lake-sediment, and they are shown below. Below that are displyed the first 5 points in the file. Finally, the points are shown plotted on the image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opkPMvq9OimD"},"outputs":[],"source":["# Open the ground reference training points\n","ground_data = gpd.read_file(\"data/training.gpkg\", layer='points')\n","\n","lc_labels = list(ground_data.land_cover_desc.unique())\n","labels = pd.DataFrame(lc_labels, ground_data.land_cover_id.unique())\n","labels.columns = ['Land-cover label']\n","labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UsTj2IFVOimD"},"outputs":[],"source":["ground_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Siv3LHf9OimD"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(14,10))\n","img_extent=[src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3]]\n","ax.use_sticky_edges = True\n","ax.imshow(bsq_to_bip(rgb_composite(rgb, 2, 1, 0)), extent=img_extent)\n","ground_data.plot(column='land_cover_id', ax=ax, alpha=0.5)"]},{"cell_type":"markdown","metadata":{"id":"GRQR6pdkOimD"},"source":["#### Training Data\n","\n","This is where we add the ground reference labels to the segments. A spatial join will add the attributes from the labeled points to the segment polygons. You can see in the few records printed below that all attributes, plus the land-cover class labels, are contained in the output table."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlctB-wzOimD"},"outputs":[],"source":["# Do geopandas spatial joins with the ground reference data\n","labeled = gpd.sjoin(vout, ground_data)\n","labeled.head()"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"j7yIpKjsOimD"},"source":["### Random Forests Classification\n","\n","#### Training and Testing Sets\n","\n","It is important to split the labeled segments into training and testing sets. First, we select a subset of all the segemnt attributes as our predictors. Second, we select the labels associated with each segment. The *train_test_split* function randomly assigns two-thirds of the points to the training set and one-third to the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvNmRRunOimE"},"outputs":[],"source":["# Names of columns to be used in the classification\n","predictors = [\"blue_mean\", \"green_mean\", \"red_mean\", \"nir_mean\", \"area\", \"perimeter\", \"major_axis_length\", \"edges_mean\"]\n","\n","# labeled is the Geopandas dataframe with attributes and labels. X is the selected columns of the data for training.\n","# y is the selected label column the corresponds to the data in X.\n","X = labeled[predictors]\n","y = labeled[[\"land_cover_id\"]]\n","\n","# This splits X and y into test and training sets. One-third of the training segments are randomly selected for the test set.\n","# Two-thirds are randomly selected for the training set.\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10)"]},{"cell_type":"markdown","metadata":{"id":"WY14py6NOimE"},"source":["#### Supervised Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOpvfiIzOimE"},"outputs":[],"source":["clf = rfc(max_depth=5, min_samples_leaf=0.05, random_state=0)\n","\n","# Fit the algorithm\n","clf.fit(X_train, np.ravel(y_train))\n","\n","# Training set prediction\n","train_prediction = clf.predict(X_train)\n","\n","# Test set prediction\n","test_prediction = clf.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"TOGQ-xtpOimE"},"source":["#### Accuracy assessment and visualiation\n","\n","Below we view the accuracy of the training and test sets. Both accuracy scores are high (training: 97.22%; testing: 89.47%), but there is nearly an 8-point difference between the two. Additional adjustments should likely be made to the training dataset and the configuration of the Random Forests classifier to minimize the difference and reduce the risk of overtraining. For learning GEOBIA now, this level of accuracy is sufficient. Below the overall accuracy scores is a dendrogram with information about the decisions learned from training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnDD3zTROimE"},"outputs":[],"source":["print(f\"Overall accuracy of the training set: {round(accuracy_score(y_train, train_prediction)*100, 2)}%\")\n","print(f\"Overall accuracy of the test set: {round(accuracy_score(y_test, test_prediction)*100, 2)}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vxv6i2p0OimE"},"outputs":[],"source":["fig = plt.figure(figsize=(50,40))\n","annotations = tree.plot_tree(clf.estimators_[0],\n","                             feature_names=predictors,\n","                             class_names=lc_labels,\n","                             filled=True)"]},{"cell_type":"markdown","metadata":{"id":"3h76Wy8xOimF"},"source":["#### Apply Learned Classifier To Full Dataset\n","\n","The final step in the GEOBIA process is to apply the learned Random Forests classifier to the entire dataset. In the cell below, we copy the vector segements and attributes to a new output file and append a new column with the label predictions for each segment. The first five segments are shown below and the full set of segments is mapped below that."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yX0t_FEtOimF"},"outputs":[],"source":["full_prediction = clf.predict(vout[predictors])\n","output = vout\n","output['prediction'] = full_prediction\n","output.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpf-MX5WOimF"},"outputs":[],"source":["# This is used to apply a custom color scheme to the figure below\n","# Dark green = forest, Light green = grassland-green, Yellow = grassland-light, Light blue = lake-deep, Dark blue = lake-sediment\n","output['color'] = output['prediction'].apply(bin_mapping)\n","\n","fig, ax = plt.subplots(figsize=(14,10))\n","img_extent=[src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3]]\n","ax.use_sticky_edges = True\n","ax.imshow(bsq_to_bip(rgb_composite(rgb, 2, 1, 0)), extent=img_extent)\n","output.plot(column='prediction', cmap=lc_cmap, ax=ax, alpha=0.6)"]},{"cell_type":"markdown","metadata":{"id":"W2NIqCzcOimF"},"source":["#### Final Interpretation\n","\n","The vector segments are displayed over the original image with a 40% transparency.\n","* Dark green is forest.\n","* Bright green is grassland-green.\n","* Yellow is grassland-light.\n","* The lighter blue is lake-deep.\n","* And the darker blue is lake-sediment.\n","\n","The test set prediction had an accuracy of approximately 90% and visually it appears that the accuracy for the whole image is similar. Forest and water areas are clearly delineated. Agricultural fields are also clearly delineated. The greenness of a grassland field falls along a spectrum of greenness and there are some fields that could be classified differently. Efforts to make those improvements would be made by adjusting the segmentation output and possibly adjusting the training data. Lastly, the water-land boundary is clearly defined. The differences in water classes are also visible in the color composite.\n","\n","#### A final comment...\n","\n","GEOBIA is really an iterative process that isn't apparent in this notebook. Segmentation is usually run many times until good parameters are found. Good parameters will be different for different images and will depend on their spatial, spectral and radiometric resolutions. They will be different for different seasons and places on the Earth. It takes time to find a good set of segmentation parameters. Calculating spectral, shape and texture attributes may not be very iterative, but finding the best combination of them in the classification process can be. Which are the best predictors of your objects of interest? Classification, too, can be very iterative as you refine training data and classification algorithm parameters. Phew! There are lots of moving parts to GEOBIA. BUT, every classification algorithm, properly executed, takes time to get just right. GEOBIA can produce excellent results when you have H-resolution."]},{"cell_type":"markdown","metadata":{"id":"KkT0s1HOOimF"},"source":["<div class=\"alert alert-danger\" role=\"alert\">\n","\n","### <a id='challenge'></a>Challenge\n","\n","[Back to top](#TOC_TOP)\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"lCAbOsdkOimF"},"source":["<div class=\"alert alert-block alert-warning\">\n","\n","#### Time to explore!\n","\n","You are encouraged to explore different segmentation algorithms available in the [scikit-image](https://scikit-image.org) package. Below we have prepared parameters for the SLIC and Felzenszwalb algorithms that you can use. Please adjust the parameters and examine the different segments produced.\n"," <div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMRBJ0nVOimF"},"outputs":[],"source":["slic_params = {\n","    'n_segments': 2500,             # default = 100\n","    'compactness': 3.5,             # default = 10.0\n","    'max_num_iter': 20,             # default = 10\n","    'sigma': 0,                     # default = 0\n","    'convert2lab': True,            # default = None, but True is highly recommended for RGB images\n","    'enforce_connectivity': True,   # default = True\n","    'min_size_factor': 0.5,         # default = 0.5\n","    'max_size_factor': 3,           # default = 3\n","    'slic_zero': False,             # default = False (if True, runs zero parameter version of SLIC)\n","    'channel_axis': 2               # for our RGB image this should always be 2\n","}\n","\n","felz_params = {\n","    'scale': 2.7,      # default = 1\n","    'sigma': 0.4,      # default = 0.8\n","    'min_size': 22,    # default = 20\n","    'channel_axis': 2  # for our RGB image this should always be 2\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1OlbVwMOimG"},"outputs":[],"source":["# SLIC\n","slic = segmentation(model=slic, params=slic_params, image=image[:3])\n","slic_vout = vectorize(image=slic, transform=src.transform, crs=src.crs.to_epsg())\n","# Felzenszwalb\n","felz = segmentation(model=felzenszwalb, params=felz_params, image=image[:3], sieve_size=30)\n","felz_vout = vectorize(image=felz, transform=src.transform, crs=src.crs.to_epsg())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGLD6xUbOimG"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(14,10))\n","img_extent=[src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3]]\n","ax.set_title(\"SLIC segmentation\")\n","ax.use_sticky_edges = True\n","ax.imshow(bsq_to_bip(rgb_composite(rgb, 2, 1, 0)), extent=img_extent)\n","slic_vout.boundary.plot(ax=ax)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQSWsv2rOimG"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(14,10))\n","img_extent=[src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3]]\n","ax.set_title(\"Felzenzswalb segmentation\")\n","ax.use_sticky_edges = True\n","ax.imshow(bsq_to_bip(rgb_composite(rgb, 2, 1, 0)), extent=img_extent)\n","felz_vout.boundary.plot(ax=ax)"]},{"cell_type":"code","source":[],"metadata":{"id":"GK0VzqoOc8x0"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"provenance":[{"file_id":"https://github.com/chikaj/7316/blob/main/geobia.ipynb","timestamp":1770672988236}]}},"nbformat":4,"nbformat_minor":0}